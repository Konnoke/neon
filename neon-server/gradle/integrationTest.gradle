/*
 * Copyright 2014 Next Century Corporation
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */
apply plugin: 'groovy'

def TEST_GROUP = JavaBasePlugin.VERIFICATION_GROUP

sourceSets {
    integrationTest {
        resources {
            srcDir 'src/integrationTest/resources'
            srcDir 'src/test-data'
        }
    }
}

configurations {
    integrationTestCompile { extendsFrom testCompile }
    integrationTestRuntime { extendsFrom integrationTestCompile, testRuntime }
}

dependencies {

    integrationTestCompile('org.apache.hadoop:hadoop-hdfs:2.0.0-cdh4.3.0') {
        exclude group: 'com.google.guava'
        exclude group: 'javax.servlet'
        exclude group: 'javax.servlet.jsp'
        exclude module: 'jasper-runtime'
        // using log4j-over-slf4j
        exclude module: 'log4j'
        exclude module: 'slf4j-log4j12'
    }

    integrationTestCompile sourceSets.main.output
    integrationTestCompile sourceSets.test.output

}

task integrationTest(type: Test) {
    description = "Runs the neon integration tests"
    group = TEST_GROUP
    dependsOn 'integrationTestClasses'

    // TODO: NEON-960 workaround
    if (isMongoOnly()) {
        dependsOn 'insertMongoDataIntegrationTest','insertMongoDataTagCloudTest'
    } else if (isSparkSQLOnly()) {
        dependsOn 'insertSparkSQLDataIntegrationTest'
    } else {
        dependsOn 'insertMongoDataIntegrationTest', 'insertMongoDataTagCloudTest', 'insertSparkSQLDataIntegrationTest'
    }

    testClassesDir = sourceSets.integrationTest.output.classesDir
    classpath = sourceSets.integrationTest.runtimeClasspath

    // allow project properties and just pass them through to the system properties
    def props = [:]
    props["mongo.host"] = getMongoHost()
    props["sparksql.host"] = getSparkSQLHost()
    props["hdfs.url"] = getHdfsUrl()

    props['integration.test'] = true
    systemProperties props
}


task insertMongoDataIntegrationTest(type: com.ncc.neon.data.MongoDataInserter) {
    group = TEST_GROUP
    description = "Inserts test data used for mongo integration tests"
    dependsOn 'deleteMongoDataIntegrationTestBeforeInsert', 'generateMongoJson'
    host = getMongoHost()
    databaseName = 'neonintegrationtest'
}

task deleteMongoDataIntegrationTestBeforeInsert(type: com.ncc.neon.data.MongoDataDeleter) {
    group = TEST_GROUP
    description = "Deletes any old integration test data from mongo that may have been left around by previous tests"
    host = getMongoHost()
    databaseName = 'neonintegrationtest'
}


// TODO: NEON-1106, NEON-1107 the tag cloud data insert/delete are only here because neon does not fully support the queries needed for the tag cloud
task insertMongoDataTagCloudTest(type: com.ncc.neon.data.MongoDataInserter) {
    dependsOn 'deleteMongoDataTagCloudTestBeforeInsert', 'generateMongoJson'
    host = getMongoHost()
    databaseName = 'neonmongotagcloud'
    dataFileName = 'tagcloud.json'
}

task deleteMongoDataTagCloudTestBeforeInsert(type: com.ncc.neon.data.MongoDataDeleter) {
    host = getMongoHost()
    databaseName = 'neonmongotagcloud'
}

task deleteMongoDataTagCloudTest(type: com.ncc.neon.data.MongoDataDeleter) {
    host = getMongoHost()
    databaseName = 'neonmongotagcloud'
}

task insertSparkSQLDataIntegrationTest(type: com.ncc.neon.data.SparkSQLDataInserter) {
    group = TEST_GROUP
    description = "Inserts test data used for spark sql integration tests"
    dependsOn 'deleteSparkSQLDataIntegrationTestBeforeInsert', 'generateSparkSQLCSV', 'generateSparkSQLJson'
    host = getSparkSQLHost()
    hdfsUrl = project.getHdfsUrl()
    databaseName = 'neonintegrationtest'
}

task deleteSparkSQLDataIntegrationTestBeforeInsert(type: com.ncc.neon.data.SparkSQLDataDeleter) {
    group = TEST_GROUP
    description = "Deletes any old integration test data from spark sql that may have been left around by previous tests"
    host = getSparkSQLHost()
    databaseName = 'neonintegrationtest'
}

task afterIntegrationTest {
    group = TEST_GROUP
    description = "Cleans up test data after the integration tests"
    // TODO: NEON-960 workaround
    if (isMongoOnly()) {
        dependsOn 'deleteMongoDataIntegrationTest', 'deleteMongoDataTagCloudTest'
    } else if (isSparkSQLOnly()) {
        dependsOn 'deleteSparkSQLDataIntegrationTest'
    } else {
        dependsOn 'deleteMongoDataIntegrationTest', 'deleteMongoDataTagCloudTest', 'deleteSparkSQLDataIntegrationTest'
    }
}

integrationTest.finalizedBy afterIntegrationTest

task deleteMongoDataIntegrationTest(type: com.ncc.neon.data.MongoDataDeleter) {
    group = TEST_GROUP
    description = "Deletes any integration test data from mongo"
    host = getMongoHost()
    databaseName = 'neonintegrationtest'
}

task deleteSparkSQLDataIntegrationTest(type: com.ncc.neon.data.SparkSQLDataDeleter) {
    group = TEST_GROUP
    description = "Deletes any integration test data from spark sql"
    host = getSparkSQLHost()
    databaseName = 'neonintegrationtest'
}

// TODO: NEON-960 The isXXXOnly are workarounds for NEON-960
def isMongoOnly() {
    // all other non-spark sql tests require mongo
    def integrationTestSingle = System.getProperty("integrationTest.single")
    return integrationTestSingle && !integrationTestSingle.toLowerCase().contains("spark")
}

def isSparkSQLOnly() {
    return System.getProperty("integrationTest.single")?.toLowerCase()?.contains("spark")
}